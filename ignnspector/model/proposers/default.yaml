
activation: relu
layers:
  # inside layer
  default_layer:
    - activation: relu

  # last layer for node classification
  last_node_clas:
    - type: MLP
      activation: log_softmax

  # last layers for graph classification
  last_graph_clas:
    - type: readout
    - type: linear